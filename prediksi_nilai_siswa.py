# -*- coding: utf-8 -*-
"""prediksi_nilai_siswa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fZV9RPmoZt8KdOx506A6rSlcoVQzx6ef

# Proyek Machine Learning : Predictive Analytics

* Domain : Pendidikan
* Tujuan : Melakukan prediksi performa siswa di pendidikan menengah di dua sekolah Portugis
* Dataset yang digunakan : https://archive.ics.uci.edu/dataset/320/student+performance

### Install *package* ucimlrepo
"""

!pip install ucimlrepo

"""### Import Library yang diperlukan"""

from ucimlrepo import fetch_ucirepo

import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import numpy as np
import warnings
import pickle
import pydotplus

from sklearn.linear_model import (
    LinearRegression,
    Ridge,
    Lasso,
    LogisticRegression,
    SGDClassifier,
    BayesianRidge,
)
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn import tree
from six import StringIO
from IPython.display import Image
from sklearn.ensemble import (
    RandomForestRegressor,
    GradientBoostingRegressor,
    BaggingClassifier,
    GradientBoostingClassifier,
    RandomForestClassifier,
)
from sklearn.svm import LinearSVC
from numpy.ma.core import sqrt
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
from numpy.polynomial.polynomial import polyfit
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    mean_squared_error,
    r2_score,
    mean_absolute_error,
    confusion_matrix
)
# Suppress FutureWarnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""## DATA UNDERSTANDING

### Load Dataset
"""

# fetch dataset
student_performance = fetch_ucirepo(id=320)

# data (as pandas dataframes)
X = student_performance.data.features
y = student_performance.data.targets

"""Kode di atas digunakan untuk memuat dataset ke dalam dua variabel X dan y, dimana X digunakan untuk data pelatihan model (variabel independen), dan y adalah nilai yang akan diprediksi oleh model berdasarkan X (variabel dependen atau label)

### Melihat ukuran Dataframe
"""

print(X.shape)
print(y.shape)

"""Kita perlu menggabungkan variabel X (Independen) dengan varibel y (Label), tapi sebelum itu kita lihat terlebih dahulu ukuran dari data tersebut. Total data baik varibel X dan y berjumlah 649, sehingga dapat kita gabungkan dengan aman

### Gabungkan Dataframe Fitur dan Target
"""

df = pd.concat([X, y], axis=1)

"""# Exploratory Data Analysis-Deskripsi Variabel

### Menampilkan tipe data setiap kolom pada dataset "df"
"""

df.info()

"""## Deskripsi Variabel Numerik"""

df.describe()

"""### Melakukan pemeriksaan terhadap nilai yang hilang(missing value) pada dataset"""

df.isnull().sum()

"""### Memvisualisasikan data menggunakan boxplot untuk fitur numerik:"""

#Melihat data outlier dengan visualisasi boxplot
num_cols = len(df.select_dtypes(include=['int64']).columns)
num_rows = (num_cols + 3) // 4  # Calculate rows needed, ensuring at least 1
df.plot(kind='box', subplots=True, layout=(num_rows, 4), figsize=(15, num_rows * 4))  # Adjust figsize for better visualization
plt.tight_layout()  # Add this line to prevent overlapping of subplots
plt.show()

"""## Exploratory Data Analysis - Univariate Analysis

Pada bagian ini kita akan melihat sebaran data pada seluruh variabel dan hubungan pada setiap variabel
"""

# Ambil semua kolom numerik
numeric_cols = df.select_dtypes(include=np.number).columns.tolist()

# Buat histogram terpisah untuk setiap kolom
fig, axes = plt.subplots(len(numeric_cols), 1, figsize=(10, 4*len(numeric_cols)))

for i, col in enumerate(numeric_cols):
  axes[i].hist(df[col], bins=20) # Anda dapat menyesuaikan jumlah bins sesuai kebutuhan
  axes[i].set_title(f'Penyebaran Data {col}')
  axes[i].set_xlabel(col)
  axes[i].set_ylabel('Frekuensi')

plt.tight_layout()
plt.show()

"""## Menganalisa data menggunakan Multivariate Analysis"""

# fitur numerik
category = df.select_dtypes(include='int64').columns.to_list()

for col in category:
  sns.catplot(x=col, y="G3", kind="bar", dodge=False, height = 4,
              aspect = 5, data=df, palette="Set3")

"""### Menampilkan Plot Pair fitur numerik"""

sns.pairplot(df, diag_kind='kde')

"""### Melakukan pengamatan terhadap tingkat korelasi dengan menggunakan matrik korelasi pada tiap fitur"""

# Memilih hanya kolom numerik
numeric_df = df.select_dtypes(include='number')

# Membuat heatmap
plt.figure(figsize=(10, 8))
correlation_matrix = numeric_df.corr().round(2)

sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidth=0.5)
plt.title("Matrik Korelasi fitur numerik", size=20)

"""## DATA PREPARATION"""

df.drop(df[df['G3'] < 1].index, inplace = True)

df_ohe = pd.get_dummies(df, drop_first=True)

# Calculate the correlation matrix for all columns
correlation_matrix = df_ohe.corr()

# Extract the correlation values for the 'G3' column
correlation_with_G3 = correlation_matrix['G3']

# Create a heatmap of the correlation values
plt.figure(figsize=(5, 13))
sns.heatmap(correlation_with_G3.to_frame(), annot=True, cmap='coolwarm', linewidth=0.5)
plt.title("Matrik Korelasi Nilai G3", size=20)

THRESHOLD = 0.13

G3_corr = df_ohe.corr()["G3"]

df_ohe_after_drop_features = df_ohe.copy()

for key, value in G3_corr.items():
  if abs(value) < THRESHOLD:
    df_ohe_after_drop_features.drop(columns= key, inplace=True)

# Calculate the correlation matrix for all columns
correlation_matrix = df_ohe_after_drop_features.corr()

# Extract the correlation values for the 'G3' column
correlation_with_G3 = correlation_matrix['G3']

# Create a heatmap of the correlation values
plt.figure(figsize=(5, 13))
sns.heatmap(correlation_with_G3.to_frame(), annot=True, cmap='coolwarm', linewidth=0.5)
plt.title("Matrik Korelasi Nilai G3", size=20)

X = df_ohe_after_drop_features.drop('G3',axis = 1)
y = df_ohe_after_drop_features['G3']

df_ohe_after_drop_features.head()

"""### Membagi dataset menjadi train dan test"""

# Dataset dibagi menjadi 80% data latih dan 20% data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)

print(f'Total of sample in whole dataset: {len(X)}')
print(f'Total of sample in train dataset: {len(X_train)}')
print(f'Total of sample in test dataset: {len(X_test)}')

"""### Menerapkan teknik Standarisasi"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#Menampilkan Hasil standarisasi dengan tabel
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)
X_train_scaled.head()

"""## MODEL DEVELOPMENT"""

X = df_ohe_after_drop_features.drop('G3',axis = 1)
y = df_ohe_after_drop_features['G3'].apply(lambda x: 'pass' if x >= 10 else 'fail')

def train_binary_classification_model(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)

    model1 = LogisticRegression(max_iter=200)
    model2 = DecisionTreeClassifier()
    model3 = KNeighborsClassifier()
    model4 = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=123, n_jobs=-1, class_weight='balanced')

    models = [model1, model2, model3, model4]
    model_name_list = ['LogisticRegression', 'DecisionTreeClassifier', 'KNeighborsClassifier', 'RandomForestClassifier']

    # Dataframe for results
    results = pd.DataFrame(columns=["Test Accuracy", "Train Accuracy"], index=model_name_list)

    # Set up subplots
    fig, axes = plt.subplots(1, len(models), figsize=(20, 4))
    fig.suptitle("Confusion Matrices for Each Model")

    for i, model in enumerate(models):
        # Train the model
        model.fit(X_train, y_train)

        # Make predictions on the test set
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        # Calculate evaluation metrics
        accuracy = accuracy_score(y_test, y_test_pred)
        accuracy_train = accuracy_score(y_train, y_train_pred)

        model_name = model_name_list[i]
        results.loc[model_name, :] = [accuracy, accuracy_train]

        # Display Confusion Matrix
        cm = confusion_matrix(y_test, y_test_pred)
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False, ax=axes[i])
        axes[i].set_title(f"{model_name}")
        axes[i].set_xlabel("Predicted")
        axes[i].set_ylabel("Actual")

        # Print Classification Report
        print(f"\nClassification Report for {model_name}:\n")
        print(classification_report(y_test, y_test_pred))

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

    return results

"""## EVALUASI MODEL"""

train_binary_classification_model(X,y)

X_all_features_except_G3 = df_ohe.drop('G3',axis = 1)
y_G3 = df_ohe ['G3'].apply(lambda x: 'pass' if x >= 10 else 'fail')

train_binary_classification_model(X_all_features_except_G3, y_G3)

"""## Kesimpulan

* Logistic Regression dan K-Nearest Neighbors lebih baik dalam hal generalisasi, karena perbedaan antara akurasi data uji dan data latih lebih kecil, sehingga kedua model ini dapat dipilih jika stabilitas dan generalisasi lebih penting.
* Decision Tree dan Random Forest menunjukkan tanda-tanda overfitting yang lebih jelas, terutama karena akurasi pada data latih mencapai 100%. Namun, peningkatan pada Decision Tree di Data 2 menunjukkan bahwa kinerjanya lebih baik dibandingkan Data 1.
* Secara keseluruhan, Random Forest memberikan akurasi uji yang paling konsisten (91.34%) pada kedua dataset, menjadikannya kandidat yang baik jika akurasi yang lebih tinggi di data uji diinginkan, meskipun ada risiko overfitting.
"""